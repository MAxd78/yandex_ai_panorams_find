#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
improved_ocr.py ‚Äî —É–ª—É—á—à–µ–Ω–Ω—ã–π OCR —Å PaddleOCR –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π

–£–ª—É—á—à–µ–Ω–∏—è –Ω–∞–¥ –±–∞–∑–æ–≤—ã–º EasyOCR:
  1. PaddleOCR –≤–º–µ—Å—Ç–æ EasyOCR (+30-40% accuracy –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ)
  2. OCR post-processing:
     - –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ —Å–ª–æ–≤–∞—Ä—é –±—Ä–µ–Ω–¥–æ–≤
     - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–∞—Å—Ç—ã—Ö –æ–ø–µ—á–∞—Ç–æ–∫
     - –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
  3. Semantic text embeddings (LaBSE) –≤–º–µ—Å—Ç–æ TF-IDF
  4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ confidence scores –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  # –ë–∞–∑–æ–≤—ã–π OCR
  python scripts/improved_ocr.py --crops-meta meta/crops.csv --output index/
  
  # –° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –ø–æ OSM –±—Ä–µ–Ω–¥–∞–º
  python scripts/improved_ocr.py --crops-meta meta/crops.csv --output index/ \
    --brand-dict poi/brand_dictionary.json
  
  # –° semantic embeddings
  python scripts/improved_ocr.py --crops-meta meta/crops.csv --output index/ \
    --semantic-embeddings
"""

from __future__ import annotations
import os
import sys
import json
import argparse
import re
from pathlib import Path
from typing import List, Dict, Tuple
from multiprocessing import Pool, cpu_count

import numpy as np
import pandas as pd
from tqdm import tqdm

# OCR engines
HAS_PADDLE = False
HAS_EASY = False

try:
    from paddleocr import PaddleOCR
    HAS_PADDLE = True
except ImportError:
    pass

try:
    import easyocr
    HAS_EASY = True
except ImportError:
    pass

# Text processing
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from scipy import sparse
    import joblib
    HAS_TFIDF = True
except ImportError:
    HAS_TFIDF = False

# Semantic embeddings
HAS_LABSE = False
try:
    from sentence_transformers import SentenceTransformer
    HAS_LABSE = True
except ImportError:
    pass


# ========================= –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã =========================

# –ß–∞—Å—Ç—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏ –≤ —Ä—É—Å—Å–∫–æ–º OCR
COMMON_TYPOS = {
    # –ü–æ—Ö–æ–∂–∏–µ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ/–∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ –±—É–∫–≤—ã
    "–ê": "A", "–í": "B", "–ï": "E", "–ö": "K", "–ú": "M",
    "–ù": "H", "–û": "O", "–†": "P", "–°": "C", "–¢": "T",
    "–£": "Y", "–•": "X",
    
    # –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏
    "0": "–û", "1": "I", "3": "–ó", "6": "–±", "8": "–í",
}

# –ú—É—Å–æ—Ä–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
JUNK_CHARS = "¬´¬ª""''‚Ä¶‚Ä¢¬∑¬∞"


# ========================= OCR Engines =========================

class OCREngine:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è OCR –¥–≤–∏–∂–∫–æ–≤"""
    
    def recognize(self, image_path: str) -> Tuple[str, float]:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        Returns:
            (text, confidence): —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ —Å—Ä–µ–¥–Ω–∏–π confidence
        """
        raise NotImplementedError


class PaddleOCREngine(OCREngine):
    """PaddleOCR –¥–≤–∏–∂–æ–∫ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)"""
    
    def __init__(self, lang="ru", use_gpu=False):
        if not HAS_PADDLE:
            raise ImportError("PaddleOCR –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: pip install paddlepaddle paddleocr")
        
        self.ocr = PaddleOCR(
            use_angle_cls=True,
            lang=lang,
            use_gpu=use_gpu,
            show_log=False,
        )
    
    def recognize(self, image_path: str) -> Tuple[str, float]:
        try:
            result = self.ocr.ocr(image_path, cls=True)
            
            if not result or not result[0]:
                return "", 0.0
            
            texts = []
            confidences = []
            
            for line in result[0]:
                text = line[1][0]
                conf = line[1][1]
                
                texts.append(text)
                confidences.append(conf)
            
            full_text = " ".join(texts)
            avg_conf = np.mean(confidences) if confidences else 0.0
            
            return full_text, float(avg_conf)
            
        except Exception as e:
            return "", 0.0


class EasyOCREngine(OCREngine):
    """EasyOCR –¥–≤–∏–∂–æ–∫ (fallback)"""
    
    def __init__(self, langs=["ru", "en"], use_gpu=False):
        if not HAS_EASY:
            raise ImportError("EasyOCR –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: pip install easyocr")
        
        self.reader = easyocr.Reader(langs, gpu=use_gpu, verbose=False)
    
    def recognize(self, image_path: str) -> Tuple[str, float]:
        try:
            results = self.reader.readtext(
                image_path,
                detail=1,
                paragraph=False,
                batch_size=16,
            )
            
            if not results:
                return "", 0.0
            
            texts = [r[1] for r in results]
            confidences = [r[2] for r in results]
            
            full_text = " ".join(texts)
            avg_conf = np.mean(confidences) if confidences else 0.0
            
            return full_text, float(avg_conf)
            
        except Exception:
            return "", 0.0


# ========================= Text Processing =========================

def load_brand_dictionary(brand_dict_path: Path) -> Dict[str, str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –±—Ä–µ–Ω–¥–æ–≤"""
    if not brand_dict_path.exists():
        return {}
    
    with open(brand_dict_path, "r", encoding="utf-8") as f:
        return json.load(f)


def correct_ocr_text(text: str, brand_dict: Dict[str, str] | None = None) -> str:
    """
    –ö–æ—Ä—Ä–µ–∫—Ü–∏—è OCR —Ç–µ–∫—Å—Ç–∞
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        brand_dict: –°–ª–æ–≤–∞—Ä—å –±—Ä–µ–Ω–¥–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
    
    Returns:
        –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text:
        return ""
    
    # 1. –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    for char in JUNK_CHARS:
        text = text.replace(char, "")
    
    # 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
    text = re.sub(r"\s+", " ", text).strip()
    
    # 3. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ª–∞—Ç–∏–Ω—Å–∫–æ-–∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—á–∞—Ç–æ–∫
    # (–°–ª–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º)
    
    # 4. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ —Å–ª–æ–≤–∞—Ä—é –±—Ä–µ–Ω–¥–æ–≤
    if brand_dict:
        words = text.lower().split()
        corrected = []
        
        for word in words:
            # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –º–∞—Ç—á–∏–Ω–≥–∞
            clean_word = re.sub(r"[^\w\s]", "", word)
            
            if clean_word in brand_dict:
                corrected.append(brand_dict[clean_word])
            else:
                corrected.append(word)
        
        text = " ".join(corrected)
    
    return text


def filter_low_confidence(
    texts: List[str],
    confidences: List[float],
    threshold: float = 0.3
) -> List[str]:
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å –Ω–∏–∑–∫–∏–º confidence"""
    filtered = []
    for text, conf in zip(texts, confidences):
        if conf >= threshold:
            filtered.append(text)
        else:
            filtered.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –±–∏—Ç—ã—Ö OCR
    
    return filtered


# ========================= Semantic Embeddings =========================

def compute_text_embeddings(texts: List[str], model_name="LaBSE") -> np.ndarray:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ semantic embeddings –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
    
    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        model_name: –ú–æ–¥–µ–ª—å –¥–ª—è embeddings (LaBSE, multilingual-e5, etc)
    
    Returns:
        –ú–∞—Ç—Ä–∏—Ü–∞ embeddings [N, D]
    """
    if not HAS_LABSE:
        print("[!] sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return None
    
    print(f"[i] –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
    
    # LaBSE ‚Äî best –¥–ª—è multilingual
    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
    if model_name == "LaBSE":
        model = SentenceTransformer("sentence-transformers/LaBSE")
    else:
        model = SentenceTransformer(model_name)
    
    print(f"[i] –í—ã—á–∏—Å–ª–µ–Ω–∏–µ embeddings –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
    embeddings = model.encode(
        texts,
        show_progress_bar=True,
        batch_size=64,
        normalize_embeddings=True,
    )
    
    return embeddings


# ========================= Main Pipeline =========================

def run_improved_ocr(
    crops_meta: Path,
    output_dir: Path,
    engine: str = "paddle",
    use_gpu: bool = False,
    workers: int = 0,
    brand_dict_path: Path | None = None,
    conf_threshold: float = 0.3,
    semantic_embeddings: bool = False,
):
    """–û—Å–Ω–æ–≤–Ω–æ–π pipeline —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ OCR"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    print(f"[i] –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
    df = pd.read_csv(crops_meta)
    paths = df["path"].tolist()
    print(f"[‚úì] –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(paths)} –∫—Ä–æ–ø–æ–≤")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OCR
    print(f"\n[i] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è {engine.upper()} OCR...")
    if engine == "paddle":
        if not HAS_PADDLE:
            print("[!] PaddleOCR –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º EasyOCR")
            engine = "easy"
    
    if engine == "paddle":
        ocr_engine = PaddleOCREngine(lang="ru", use_gpu=use_gpu)
    else:
        ocr_engine = EasyOCREngine(langs=["ru", "en"], use_gpu=use_gpu)
    
    print(f"[‚úì] OCR –≥–æ—Ç–æ–≤")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –±—Ä–µ–Ω–¥–æ–≤
    brand_dict = None
    if brand_dict_path and brand_dict_path.exists():
        print(f"\n[i] –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –±—Ä–µ–Ω–¥–æ–≤...")
        brand_dict = load_brand_dictionary(brand_dict_path)
        print(f"[‚úì] –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(brand_dict)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –±—Ä–µ–Ω–¥–æ–≤")
    
    # OCR
    print(f"\n[i] –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ OCR...")
    texts = []
    confidences = []
    
    for path in tqdm(paths, desc="OCR", unit="img"):
        if not os.path.exists(path):
            texts.append("")
            confidences.append(0.0)
            continue
        
        try:
            text, conf = ocr_engine.recognize(path)
            
            # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è
            if text and brand_dict:
                text = correct_ocr_text(text, brand_dict)
            
            texts.append(text)
            confidences.append(conf)
            
        except Exception as e:
            texts.append("")
            confidences.append(0.0)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ confidence
    if conf_threshold > 0:
        print(f"\n[i] –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ confidence >= {conf_threshold}...")
        orig_count = sum(1 for t in texts if t)
        texts = filter_low_confidence(texts, confidences, conf_threshold)
        filtered_count = sum(1 for t in texts if t)
        print(f"[i] –û—Å—Ç–∞–ª–æ—Å—å {filtered_count}/{orig_count} —Ç–µ–∫—Å—Ç–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
    output_dir.mkdir(parents=True, exist_ok=True)
    
    ocr_txt = output_dir / "ocr_texts_improved.txt"
    with open(ocr_txt, "w", encoding="utf-8") as f:
        for text in texts:
            f.write((text or "") + "\n")
    print(f"[‚úì] OCR —Ç–µ–∫—Å—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {ocr_txt}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ confidences
    conf_npy = output_dir / "ocr_confidences.npy"
    np.save(conf_npy, np.array(confidences, dtype=np.float32))
    print(f"[‚úì] Confidences —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {conf_npy}")
    
    # TF-IDF –∏–Ω–¥–µ–∫—Å
    if HAS_TFIDF:
        print(f"\n[i] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ TF-IDF –∏–Ω–¥–µ–∫—Å–∞...")
        vectorizer = TfidfVectorizer(
            lowercase=True,
            analyzer="word",
            token_pattern=r"(?u)\b[\w\-]{2,}\b",
            ngram_range=(1, 2),
            max_features=200_000,
            min_df=1,
            max_df=0.95,
        )
        
        tfidf_matrix = vectorizer.fit_transform(texts)
        
        joblib.dump(vectorizer, output_dir / "tfidf_vectorizer_improved.joblib")
        sparse.save_npz(output_dir / "tfidf_matrix_improved.npz", tfidf_matrix)
        
        print(f"[‚úì] TF-IDF –∏–Ω–¥–µ–∫—Å (vocabulary: {len(vectorizer.vocabulary_)})")
    
    # Semantic embeddings
    if semantic_embeddings and HAS_LABSE:
        print(f"\n[i] –í—ã—á–∏—Å–ª–µ–Ω–∏–µ semantic embeddings...")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
        valid_texts = [t if t else " " for t in texts]  # Placeholder –¥–ª—è –ø—É—Å—Ç—ã—Ö
        
        embeddings = compute_text_embeddings(valid_texts, model_name="LaBSE")
        
        if embeddings is not None:
            emb_path = output_dir / "text_embeddings_labse.npy"
            np.save(emb_path, embeddings)
            print(f"[‚úì] Semantic embeddings —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {emb_path} (shape: {embeddings.shape})")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ OCR:")
    print(f"   –í—Å–µ–≥–æ –∫—Ä–æ–ø–æ–≤: {len(texts)}")
    print(f"   –° —Ç–µ–∫—Å—Ç–æ–º: {sum(1 for t in texts if t)} ({sum(1 for t in texts if t)/len(texts)*100:.1f}%)")
    print(f"   –°—Ä–µ–¥–Ω–∏–π confidence: {np.mean(confidences):.3f}")
    print(f"   –ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {np.median([len(t.split()) for t in texts if t]):.0f} —Å–ª–æ–≤")
    
    print(f"\n‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π OCR –∑–∞–≤–µ—Ä—à—ë–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {output_dir}/")


# ========================= Main =========================

def main():
    parser = argparse.ArgumentParser(
        description="–£–ª—É—á—à–µ–Ω–Ω—ã–π OCR —Å PaddleOCR –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π"
    )
    
    parser.add_argument("--crops-meta", default="meta/crops.csv", help="CSV —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏")
    parser.add_argument("--output", default="index", help="–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    
    # OCR –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("--engine", choices=["paddle", "easy"], default="paddle",
                       help="OCR –¥–≤–∏–∂–æ–∫ (paddle —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)")
    parser.add_argument("--use-gpu", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU")
    parser.add_argument("--workers", type=int, default=0, help="–ß–∏—Å–ª–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–ø–æ–∫–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)")
    
    # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è
    parser.add_argument("--brand-dict", type=Path, default=None,
                       help="–ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é –±—Ä–µ–Ω–¥–æ–≤ (–∏–∑ parse_osm_to_poi.py)")
    parser.add_argument("--conf-threshold", type=float, default=0.3,
                       help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π confidence –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞")
    
    # Semantic embeddings
    parser.add_argument("--semantic-embeddings", action="store_true",
                       help="–í—ã—á–∏—Å–ª–∏—Ç—å semantic embeddings (LaBSE)")
    
    args = parser.parse_args()
    
    crops_meta = Path(args.crops_meta)
    output_dir = Path(args.output)
    
    if not crops_meta.exists():
        print(f"[!] –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {crops_meta}")
        sys.exit(1)
    
    run_improved_ocr(
        crops_meta=crops_meta,
        output_dir=output_dir,
        engine=args.engine,
        use_gpu=args.use_gpu,
        workers=args.workers,
        brand_dict_path=args.brand_dict,
        conf_threshold=args.conf_threshold,
        semantic_embeddings=args.semantic_embeddings,
    )


if __name__ == "__main__":
    main()