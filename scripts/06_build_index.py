#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
06_build_index.py â€” Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð² Ð´Ð»Ñ Ð³ÐµÐ¾Ð»Ð¾ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸

Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚:
1. HNSW Ð¸Ð½Ð´ÐµÐºÑ Ð¸Ð· fine-tuned embeddings
2. TF-IDF Ð¸Ð½Ð´ÐµÐºÑ Ð¸Ð· OCR Ñ‚ÐµÐºÑÑ‚Ð¾Ð²
3. OSM spatial index (BallTree) Ð´Ð»Ñ POI

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ:
python scripts/06_build_index.py \
    --model-path models/clip_gem/best_model.pt \
    --crops-meta meta/crops_with_ocr.csv \
    --osm-data data/osm_places.jsonl \
    --output-dir index
"""

import os
import sys
import argparse
import json
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import torch
import torch.nn.functional as F
from PIL import Image
import hnswlib
import open_clip
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import BallTree
from scipy import sparse
import joblib

# ========================= GeM Model Loading =========================

class GeM(torch.nn.Module):
    """GeM pooling (ÐºÐ¾Ð¿Ð¸Ñ Ð¸Ð· 05_train_model.py)"""
    def __init__(self, p=3.0, eps=1e-6, learn_p=True):
        super().__init__()
        self.p = torch.nn.Parameter(torch.ones(1) * p) if learn_p else p
        self.eps = eps
        
    def forward(self, x):
        return F.avg_pool2d(
            x.clamp(min=self.eps).pow(self.p),
            (x.size(-2), x.size(-1))
        ).pow(1.0 / self.p).squeeze(-1).squeeze(-1)

class CLIPGeM(torch.nn.Module):
    """CLIP + GeM (ÐºÐ¾Ð¿Ð¸Ñ Ð¸Ð· 05_train_model.py)"""
    def __init__(self, clip_model, gem_p=3.0):
        super().__init__()
        self.visual = clip_model.visual
        self.gem = GeM(p=gem_p, learn_p=True)
        
    def forward(self, x):
        # Feature extraction
        x = self.visual.conv1(x)
        x = x.reshape(x.shape, x.shape, -1)
        x = x.permute(0, 2, 1)
        x = torch.cat([self.visual.class_embedding.to(x.dtype) + \
                      torch.zeros(x.shape, 1, x.shape[-1], dtype=x.dtype, device=x.device), 
                      x], dim=1)
        x = x + self.visual.positional_embedding.to(x.dtype)
        x = self.visual.ln_pre(x)
        
        x = x.permute(1, 0, 2)
        x = self.visual.transformer(x)
        x = x.permute(1, 0, 2)
        
        # Reshape Ð´Ð»Ñ GeM
        x = x[:, 1:, :]
        B, HW, C = x.shape
        H = W = int(np.sqrt(HW))
        x = x.transpose(1, 2).reshape(B, C, H, W)
        
        pooled = self.gem(x)
        return F.normalize(pooled, p=2, dim=1)

def load_finetuned_model(checkpoint_path, model_name="ViT-L-14", pretrained="openai", device='cuda'):
    """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° fine-tuned Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
    print(f"[i] Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸: {checkpoint_path}")
    
    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ CLIP
    clip_model, _, preprocess = open_clip.create_model_and_transforms(
        model_name, pretrained=pretrained
    )
    
    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ CLIPGeM
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    gem_p = checkpoint.get('gem_p', 3.0)
    
    model = CLIPGeM(clip_model, gem_p=gem_p)
    
    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð²ÐµÑÐ°
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model = model.to(device)
    model.eval()
    
    print(f"[âœ“] ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° (GeM p={gem_p:.2f})")
    return model, preprocess

# ========================= Embeddings Extraction =========================

@torch.no_grad()
def extract_embeddings(model, crops_df, preprocess, device, batch_size=128):
    """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ embeddings Ð´Ð»Ñ Ð²ÑÐµÑ… ÐºÑ€Ð¾Ð¿Ð¾Ð²"""
    model.eval()
    
    embeddings = []
    valid_indices = []
    
    # Ð‘Ð°Ñ‚Ñ‡-Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³
    n_batches = (len(crops_df) + batch_size - 1) // batch_size
    
    for batch_idx in tqdm(range(n_batches), desc="Extracting embeddings", unit="batch"):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(crops_df))
        batch_df = crops_df.iloc[start_idx:end_idx]
        
        batch_images = []
        batch_valid = []
        
        for idx, row in batch_df.iterrows():
            try:
                img = Image.open(row['path']).convert('RGB')
                img_tensor = preprocess(img)
                batch_images.append(img_tensor)
                batch_valid.append(idx)
            except Exception as e:
                print(f"[!] Error loading {row['path']}: {e}")
                continue
        
        if not batch_images:
            continue
        
        # Forward pass
        batch_tensor = torch.stack(batch_images).to(device)
        batch_embs = model(batch_tensor)
        
        embeddings.append(batch_embs.cpu().numpy())
        valid_indices.extend(batch_valid)
    
    # Concatenate
    embeddings = np.vstack(embeddings)
    
    print(f"[âœ“] Extracted {len(embeddings)} embeddings (dim={embeddings.shape})")
    return embeddings, valid_indices

# ========================= HNSW Index =========================

def build_hnsw_index(embeddings, output_path, M=32, efC=200):
    """ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ HNSW Ð¸Ð½Ð´ÐµÐºÑÐ°"""
    print(f"\n[i] ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ HNSW Ð¸Ð½Ð´ÐµÐºÑÐ°...")
    print(f"    Vectors: {len(embeddings)}, Dim: {embeddings.shape}")
    print(f"    M={M}, efConstruction={efC}")
    
    N, D = embeddings.shape
    
    # Normalize
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-9
    embeddings = (embeddings / norms).astype(np.float32)
    
    # Create index
    index = hnswlib.Index(space='cosine', dim=D)
    index.init_index(max_elements=N, M=M, ef_construction=efC, random_seed=42)
    
    # Add items in batches
    batch_size = 10000
    for i in tqdm(range(0, N, batch_size), desc="Adding to HNSW", unit="batch"):
        end = min(i + batch_size, N)
        index.add_items(embeddings[i:end], np.arange(i, end))
    
    # Save
    index.save_index(str(output_path))
    print(f"[âœ“] HNSW saved: {output_path}")
    
    return index

# ========================= TF-IDF Index =========================

def build_tfidf_index(crops_df, output_dir):
    """ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ TF-IDF Ð¸Ð½Ð´ÐµÐºÑÐ° Ð¸Ð· OCR Ñ‚ÐµÐºÑÑ‚Ð¾Ð²"""
    print(f"\n[i] ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ TF-IDF Ð¸Ð½Ð´ÐµÐºÑÐ°...")
    
    if 'ocr_text' not in crops_df.columns:
        print("[!] ÐÐµÑ‚ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ ocr_text, Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ TF-IDF")
        return None, None
    
    texts = crops_df['ocr_text'].fillna('').tolist()
    
    # ÐŸÐ¾Ð´ÑÑ‡Ñ‘Ñ‚ Ð½ÐµÐ¿ÑƒÑÑ‚Ñ‹Ñ… Ñ‚ÐµÐºÑÑ‚Ð¾Ð²
    n_nonempty = sum(1 for t in texts if len(t.strip()) > 0)
    print(f"    Texts: {len(texts)}, Non-empty: {n_nonempty}")
    
    if n_nonempty < 10:
        print("[!] Ð¡Ð»Ð¸ÑˆÐºÐ¾Ð¼ Ð¼Ð°Ð»Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð², Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ TF-IDF")
        return None, None
    
    # Build vectorizer
    vectorizer = TfidfVectorizer(
        max_features=1000,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.8,
        lowercase=True,
        token_pattern=r'\b\w+\b'
    )
    
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    print(f"[âœ“] TF-IDF: vocab size={len(vectorizer.vocabulary_)}, matrix shape={tfidf_matrix.shape}")
    
    # Save
    vectorizer_path = output_dir / "tfidf_vectorizer.joblib"
    matrix_path = output_dir / "tfidf_matrix.npz"
    texts_path = output_dir / "ocr_texts.txt"
    
    joblib.dump(vectorizer, vectorizer_path)
    sparse.save_npz(matrix_path, tfidf_matrix)
    
    with open(texts_path, 'w', encoding='utf-8') as f:
        for text in texts:
            f.write(text + '\n')
    
    print(f"[âœ“] TF-IDF saved:")
    print(f"    Vectorizer: {vectorizer_path}")
    print(f"    Matrix: {matrix_path}")
    
    return vectorizer, tfidf_matrix

# ========================= OSM Spatial Index =========================

def build_osm_spatial_index(osm_data_path, output_path):
    """ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ spatial index Ð´Ð»Ñ OSM POI"""
    print(f"\n[i] ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ OSM spatial index...")
    
    if not Path(osm_data_path).exists():
        print(f"[!] OSM data Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {osm_data_path}")
        print("    ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ OSM index")
        return None
    
    # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° OSM POI
    pois = []
    with open(osm_data_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                pois.append(json.loads(line))
    
    print(f"[i] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(pois)} POI")
    
    if len(pois) == 0:
        print("[!] ÐÐµÑ‚ POI Ð´Ð°Ð½Ð½Ñ‹Ñ…")
        return None
    
    # Extract coordinates
    coords = np.array([[poi['lat'], poi['lon']] for poi in pois])
    
    # Build BallTree Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ñ€Ð°Ð´Ð¸ÑƒÑÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°
    tree = BallTree(np.radians(coords), metric='haversine')
    
    print(f"[âœ“] BallTree Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½: {len(coords)} points")
    
    # Save
    spatial_data = {
        'tree': tree,
        'pois': pois,
        'coords': coords
    }
    joblib.dump(spatial_data, output_path)
    
    print(f"[âœ“] OSM spatial index saved: {output_path}")
    
    return spatial_data

# ========================= Main =========================

def main():
    parser = argparse.ArgumentParser(
        description="ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð² Ð´Ð»Ñ Ð³ÐµÐ¾Ð»Ð¾ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸"
    )
    
    # Ð’Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    parser.add_argument("--model-path", required=True,
                       help="Path Ðº fine-tuned Ð¼Ð¾Ð´ÐµÐ»Ð¸ (.pt Ñ„Ð°Ð¹Ð»)")
    parser.add_argument("--crops-meta", required=True,
                       help="CSV Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ ÐºÑ€Ð¾Ð¿Ð¾Ð²")
    parser.add_argument("--osm-data", default=None,
                       help="JSONL Ñ„Ð°Ð¹Ð» Ñ OSM POI (optional)")
    
    # Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð°Ñ Ð¿Ð°Ð¿ÐºÐ°
    parser.add_argument("--output-dir", default="index",
                       help="ÐŸÐ°Ð¿ÐºÐ° Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð²")
    
    # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
    parser.add_argument("--model-name", default="ViT-L-14",
                       help="CLIP Ð¼Ð¾Ð´ÐµÐ»ÑŒ (default: ViT-L-14)")
    parser.add_argument("--pretrained", default="openai",
                       help="Pretrained source")
    
    # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð²
    parser.add_argument("--batch-size", type=int, default=128,
                       help="Batch size Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ embeddings")
    parser.add_argument("--hnsw-M", type=int, default=32,
                       help="HNSW parameter M")
    parser.add_argument("--hnsw-efC", type=int, default=200,
                       help="HNSW efConstruction")
    
    args = parser.parse_args()
    
    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"[i] Device: {device}")
    
    # Output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ========== 1. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ ==========
    print("\n[1/5] Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° fine-tuned Ð¼Ð¾Ð´ÐµÐ»Ð¸")
    model, preprocess = load_finetuned_model(
        args.model_path,
        model_name=args.model_name,
        pretrained=args.pretrained,
        device=device
    )
    
    # ========== 2. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… ==========
    print(f"\n[2/5] Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…")
    crops_df = pd.read_csv(args.crops_meta)
    
    # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð²
    existing_mask = crops_df['path'].apply(lambda p: Path(p).exists())
    crops_df = crops_df[existing_mask].reset_index(drop=True)
    
    print(f"[âœ“] {len(crops_df)} crops")
    
    # ========== 3. Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ embeddings ==========
    print(f"\n[3/5] Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ embeddings")
    embeddings, valid_indices = extract_embeddings(
        model, crops_df, preprocess, device, batch_size=args.batch_size
    )
    
    # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ crops_df Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¼Ð¸
    crops_df = crops_df.iloc[valid_indices].reset_index(drop=True)
    
    # Save embeddings
    embs_path = output_dir / "clip_gem_embeddings.npy"
    np.save(embs_path, embeddings)
    print(f"[âœ“] Embeddings saved: {embs_path}")
    
    # Save metadata
    meta_path = output_dir / "crops.csv"
    crops_df.to_csv(meta_path, index=False)
    print(f"[âœ“] Metadata saved: {meta_path}")
    
    # ========== 4. ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ HNSW Ð¸Ð½Ð´ÐµÐºÑÐ° ==========
    print(f"\n[4/5] ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ HNSW Ð¸Ð½Ð´ÐµÐºÑÐ°")
    hnsw_path = output_dir / "hnsw_gem.bin"
    build_hnsw_index(embeddings, hnsw_path, M=args.hnsw_M, efC=args.hnsw_efC)
    
    # ========== 5. ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ TF-IDF Ð¸Ð½Ð´ÐµÐºÑÐ° ==========
    build_tfidf_index(crops_df, output_dir)
    
    # ========== 6. ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ OSM spatial index ==========
    if args.osm_data:
        osm_path = output_dir / "osm_spatial.pkl"
        build_osm_spatial_index(args.osm_data, osm_path)
    
    # ========== Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸ ==========
    config = {
        'model_name': args.model_name,
        'pretrained': args.pretrained,
        'model_path': str(args.model_path),
        'embedding_dim': int(embeddings.shape),
        'n_crops': len(crops_df),
        'hnsw_M': args.hnsw_M,
        'hnsw_efC': args.hnsw_efC,
    }
    
    config_path = output_dir / "index_config.json"
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"\n{'='*60}")
    print("âœ… ÐŸÐžÐ¡Ð¢Ð ÐžÐ•ÐÐ˜Ð• Ð˜ÐÐ”Ð•ÐšÐ¡ÐžÐ’ Ð—ÐÐ’Ð•Ð Ð¨Ð•ÐÐž")
    print(f"{'='*60}")
    print(f"Ð˜Ð½Ð´ÐµÐºÑÑ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð²: {output_dir}/")
    print(f"  - HNSW: hnsw_gem.bin")
    print(f"  - Embeddings: clip_gem_embeddings.npy")
    print(f"  - TF-IDF: tfidf_*.joblib/npz")
    print(f"  - OSM: osm_spatial.pkl")
    print(f"  - Config: index_config.json")
    
    print("\nðŸŽ¯ Ð¡Ð»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³:")
    print(f"   python scripts/07_query_improved.py --image samples/test.jpg")

if __name__ == "__main__":
    main()
