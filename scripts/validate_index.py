#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
validate_index.py ‚Äî –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞

–§—É–Ω–∫—Ü–∏–∏:
  ‚úÖ Self-testing: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—Ä–æ–ø—ã –∫–∞–∫ query –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å
  ‚úÖ Quality metrics: —Å—Ä–µ–¥–Ω—è—è similarity, –≥–µ–æ–º–µ—Ç—Ä–∏—è, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
  ‚úÖ Auto-tuning: –ø–æ–¥–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (geom_weight, verify_k)
  ‚úÖ Benchmark: –∑–∞–º–µ—Ä—è–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
  python scripts/validate_index.py

  # –° auto-tuning –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
  python scripts/validate_index.py --auto-tune

  # –¢–æ–ª—å–∫–æ –±—ã—Å—Ç—Ä—ã–µ —Ç–µ—Å—Ç—ã
  python scripts/validate_index.py --quick
"""

import os
import sys
import json
import argparse
import random
from pathlib import Path
from typing import List, Dict, Tuple
import time

import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt

import torch
import hnswlib

# –ü–æ–¥–∞–≤–ª—è–µ–º –≤–∞—Ä–Ω–∏–Ω–≥–∏
import warnings
warnings.filterwarnings("ignore")

# ========================= –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã =========================
SEED = 42
DEFAULT_TEST_SIZE = 100  # –ö—Ä–æ–ø–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
DEFAULT_QUICK_SIZE = 20

# ========================= Utils =========================

def load_index(index_dir: Path) -> Tuple:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
    print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑: {index_dir}")
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    meta_parquet = index_dir / "crops.parquet"
    if meta_parquet.exists():
        meta = pd.read_parquet(meta_parquet)
    else:
        meta = pd.read_csv("meta/crops.csv")
    
    print(f"   –ö—Ä–æ–ø–æ–≤ –≤ –º–µ—Ç–∞: {len(meta)}")
    
    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
    embs_file = None
    for candidate in ["embs.npy", "embeddings.npy", "clip_embeddings.npy"]:
        f = index_dir / candidate
        if f.exists():
            embs_file = f
            break
    
    if embs_file is None:
        raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤!")
    
    embs = np.load(embs_file)
    print(f"   –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: {embs.shape}")
    
    # HNSW
    hnsw_file = None
    for candidate in ["hnsw.bin", "hnsw_clip.bin"]:
        f = index_dir / candidate
        if f.exists():
            hnsw_file = f
            break
    
    if hnsw_file is None:
        raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω HNSW –∏–Ω–¥–µ–∫—Å!")
    
    dim = embs.shape[1]
    index = hnswlib.Index(space="cosine", dim=dim)
    index.load_index(str(hnsw_file))
    print(f"   HNSW: {hnsw_file.name}")
    
    # Model config
    model_json = index_dir / "model.json"
    if model_json.exists():
        with open(model_json) as f:
            model_config = json.load(f)
        print(f"   –ú–æ–¥–µ–ª—å: {model_config.get('model', 'unknown')}")
    else:
        model_config = {}
    
    return meta, embs, index, model_config


def sample_test_set(meta: pd.DataFrame, size: int) -> pd.DataFrame:
    """–í—ã–±—Ä–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ –∫—Ä–æ–ø—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    random.seed(SEED)
    indices = random.sample(range(len(meta)), min(size, len(meta)))
    return meta.iloc[indices].reset_index(drop=True)


def cosine_to_sim(dist: np.ndarray) -> np.ndarray:
    return 1.0 - dist


# ========================= Tests =========================

class IndexValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –∏–Ω–¥–µ–∫—Å–∞"""
    
    def __init__(self, meta, embs, index, model_config):
        self.meta = meta
        self.embs = embs
        self.index = index
        self.model_config = model_config
        
        self.results = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "metrics": {},
        }
    
    def test_self_retrieval(self, test_set: pd.DataFrame, topk: int = 50) -> Dict:
        """
        –¢–µ—Å—Ç —Å–∞–º–æ–ø–æ–∏—Å–∫–∞: –∫–∞–∂–¥—ã–π –∫—Ä–æ–ø –¥–æ–ª–∂–µ–Ω –Ω–∞–π—Ç–∏ —Å–∞–º —Å–µ–±—è –≤ —Ç–æ–ø-1
        """
        print("\n" + "="*80)
        print("üîç –¢–ï–°–¢ 1: Self-Retrieval (–∫—Ä–æ–ø –Ω–∞—Ö–æ–¥–∏—Ç —Å–∞–º —Å–µ–±—è)")
        print("="*80)
        
        self.index.set_ef(200)
        
        correct_top1 = 0
        correct_top5 = 0
        correct_top10 = 0
        similarities = []
        ranks = []
        
        for idx, row in tqdm(test_set.iterrows(), total=len(test_set), desc="Self-retrieval"):
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∫—Ä–æ–ø–∞
            crop_idx = meta[meta["crop_id"] == row["crop_id"]].index[0]
            q_emb = self.embs[crop_idx:crop_idx+1]
            
            # –ü–æ–∏—Å–∫
            labels, dists = self.index.knn_query(q_emb, k=topk)
            labels = labels[0]
            sims = cosine_to_sim(dists[0])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Å–∞–º –∫—Ä–æ–ø
            if crop_idx in labels:
                rank = np.where(labels == crop_idx)[0][0] + 1
                ranks.append(rank)
                
                if rank == 1:
                    correct_top1 += 1
                if rank <= 5:
                    correct_top5 += 1
                if rank <= 10:
                    correct_top10 += 1
                
                similarities.append(sims[rank-1])
            else:
                ranks.append(topk + 1)
                similarities.append(0.0)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        total = len(test_set)
        top1_acc = correct_top1 / total * 100
        top5_acc = correct_top5 / total * 100
        top10_acc = correct_top10 / total * 100
        avg_sim = np.mean(similarities) if similarities else 0
        avg_rank = np.mean(ranks)
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   Top-1 —Ç–æ—á–Ω–æ—Å—Ç—å: {top1_acc:.1f}% ({correct_top1}/{total})")
        print(f"   Top-5 —Ç–æ—á–Ω–æ—Å—Ç—å: {top5_acc:.1f}% ({correct_top5}/{total})")
        print(f"   Top-10 —Ç–æ—á–Ω–æ—Å—Ç—å: {top10_acc:.1f}% ({correct_top10}/{total})")
        print(f"   –°—Ä–µ–¥–Ω—è—è similarity: {avg_sim:.4f}")
        print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–Ω–≥: {avg_rank:.1f}")
        
        # –û—Ü–µ–Ω–∫–∞
        if top1_acc >= 95:
            print("   ‚úÖ –û–¢–õ–ò–ß–ù–û! –ò–Ω–¥–µ–∫—Å —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–¥–µ–∞–ª—å–Ω–æ")
        elif top1_acc >= 85:
            print("   ‚úÖ –•–û–†–û–®–û! –ò–Ω–¥–µ–∫—Å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        elif top1_acc >= 70:
            print("   ‚ö†Ô∏è  –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û. –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π")
        else:
            print("   ‚ùå –ü–õ–û–•–û! –ò–Ω–¥–µ–∫—Å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ!")
        
        self.results["test_self_retrieval"] = {
            "top1_acc": top1_acc,
            "top5_acc": top5_acc,
            "top10_acc": top10_acc,
            "avg_similarity": float(avg_sim),
            "avg_rank": float(avg_rank),
        }
        
        return self.results["test_self_retrieval"]
    
    def test_same_pano_retrieval(self, test_set: pd.DataFrame, topk: int = 50) -> Dict:
        """
        –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ —Ç–æ–π –∂–µ –ø–∞–Ω–æ—Ä–∞–º—ã: –∫—Ä–æ–ø—ã –æ–¥–Ω–æ–π –ø–∞–Ω–æ—Ä–∞–º—ã –¥–æ–ª–∂–Ω—ã –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –±–ª–∏–∑–∫–æ
        """
        print("\n" + "="*80)
        print("üîç –¢–ï–°–¢ 2: Same-Pano Retrieval (–Ω–∞—Ö–æ–¥–∏—Ç –∫—Ä–æ–ø—ã —Ç–æ–π –∂–µ –ø–∞–Ω–æ—Ä–∞–º—ã)")
        print("="*80)
        
        self.index.set_ef(200)
        
        same_pano_in_top5 = 0
        same_pano_in_top10 = 0
        avg_same_pano_count = []
        
        for idx, row in tqdm(test_set.iterrows(), total=len(test_set), desc="Same-pano"):
            crop_idx = meta[meta["crop_id"] == row["crop_id"]].index[0]
            pano_id = row["pano_id"]
            
            q_emb = self.embs[crop_idx:crop_idx+1]
            labels, dists = self.index.knn_query(q_emb, k=topk)
            labels = labels[0]
            
            # –°–∫–æ–ª—å–∫–æ –∫—Ä–æ–ø–æ–≤ —Ç–æ–π –∂–µ –ø–∞–Ω–æ—Ä–∞–º—ã –≤ —Ç–æ–ø-K
            same_pano_labels = meta.iloc[labels]["pano_id"] == pano_id
            same_count_top50 = same_pano_labels.sum()
            same_count_top5 = same_pano_labels[:5].sum()
            same_count_top10 = same_pano_labels[:10].sum()
            
            avg_same_pano_count.append(same_count_top50)
            
            if same_count_top5 >= 2:  # –ú–∏–Ω–∏–º—É–º 2 –∫—Ä–æ–ø–∞ (—Å–∞–º + –µ—â—ë –æ–¥–∏–Ω)
                same_pano_in_top5 += 1
            if same_count_top10 >= 3:
                same_pano_in_top10 += 1
        
        total = len(test_set)
        top5_rate = same_pano_in_top5 / total * 100
        top10_rate = same_pano_in_top10 / total * 100
        avg_count = np.mean(avg_same_pano_count)
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   –ö—Ä–æ–ø—ã —Ç–æ–π –∂–µ –ø–∞–Ω–æ—Ä–∞–º—ã –≤ Top-5: {top5_rate:.1f}%")
        print(f"   –ö—Ä–æ–ø—ã —Ç–æ–π –∂–µ –ø–∞–Ω–æ—Ä–∞–º—ã –≤ Top-10: {top10_rate:.1f}%")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª-–≤–æ –∫—Ä–æ–ø–æ–≤ –≤ Top-50: {avg_count:.1f}")
        
        if top5_rate >= 80:
            print("   ‚úÖ –û–¢–õ–ò–ß–ù–û! –ö—Ä–æ–ø—ã –ø–∞–Ω–æ—Ä–∞–º –≥—Ä—É–ø–ø–∏—Ä—É—é—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
        elif top5_rate >= 60:
            print("   ‚úÖ –•–û–†–û–®–û!")
        else:
            print("   ‚ö†Ô∏è  –°–õ–ê–ë–û. –í–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –∫—Ä–æ–ø–æ–≤ –Ω–∞ –ø–∞–Ω–æ—Ä–∞–º—É")
        
        self.results["test_same_pano"] = {
            "top5_rate": top5_rate,
            "top10_rate": top10_rate,
            "avg_same_pano_count": float(avg_count),
        }
        
        return self.results["test_same_pano"]
    
    def test_similarity_distribution(self) -> Dict:
        """
        –¢–µ—Å—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è similarity: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã
        """
        print("\n" + "="*80)
        print("üìä –¢–ï–°–¢ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Similarity")
        print("="*80)
        
        # –ë–µ—Ä—ë–º —Å–ª—É—á–∞–π–Ω—ã–µ 500 —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        sample_size = min(500, len(self.embs))
        indices = random.sample(range(len(self.embs)), sample_size)
        sample_embs = self.embs[indices]
        
        # –°—á–∏—Ç–∞–µ–º –Ω–æ—Ä–º—ã
        norms = np.linalg.norm(sample_embs, axis=1)
        avg_norm = float(np.mean(norms))
        std_norm = float(np.std(norms))
        
        print(f"\nüìä –ù–æ—Ä–º—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ: {avg_norm:.6f}")
        print(f"   Std: {std_norm:.6f}")
        print(f"   Min: {norms.min():.6f}")
        print(f"   Max: {norms.max():.6f}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        is_normalized = (0.99 <= avg_norm <= 1.01) and (std_norm < 0.01)
        
        if is_normalized:
            print("   ‚úÖ –û–¢–õ–ò–ß–ù–û! –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã")
        else:
            print("   ‚ö†Ô∏è  –ü–†–û–ë–õ–ï–ú–ê! –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ)")
        
        self.results["test_similarity_dist"] = {
            "avg_norm": avg_norm,
            "std_norm": std_norm,
            "is_normalized": is_normalized,
        }
        
        return self.results["test_similarity_dist"]
    
    def benchmark_speed(self, test_set: pd.DataFrame, topk: int = 50) -> Dict:
        """
        Benchmark —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞
        """
        print("\n" + "="*80)
        print("‚ö° BENCHMARK: –°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞")
        print("="*80)
        
        self.index.set_ef(200)
        
        times = []
        
        for idx, row in tqdm(test_set.iterrows(), total=len(test_set), desc="Benchmark"):
            crop_idx = meta[meta["crop_id"] == row["crop_id"]].index[0]
            q_emb = self.embs[crop_idx:crop_idx+1]
            
            start = time.time()
            labels, dists = self.index.knn_query(q_emb, k=topk)
            elapsed = time.time() - start
            
            times.append(elapsed * 1000)  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
        
        avg_time = np.mean(times)
        p50_time = np.percentile(times, 50)
        p95_time = np.percentile(times, 95)
        p99_time = np.percentile(times, 99)
        
        queries_per_sec = 1000 / avg_time if avg_time > 0 else 0
        
        print(f"\n‚è±Ô∏è  –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ (k={topk}):")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ: {avg_time:.2f} ms")
        print(f"   P50: {p50_time:.2f} ms")
        print(f"   P95: {p95_time:.2f} ms")
        print(f"   P99: {p99_time:.2f} ms")
        print(f"   Queries/sec: {queries_per_sec:.1f}")
        
        if avg_time < 50:
            print("   ‚ö° –û–¢–õ–ò–ß–ù–û! –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ")
        elif avg_time < 100:
            print("   ‚úÖ –•–û–†–û–®–û!")
        else:
            print("   ‚ö†Ô∏è  –ú–ï–î–õ–ï–ù–ù–û. –í–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å ef –∏–ª–∏ M")
        
        self.results["benchmark"] = {
            "avg_time_ms": float(avg_time),
            "p95_time_ms": float(p95_time),
            "queries_per_sec": float(queries_per_sec),
        }
        
        return self.results["benchmark"]
    
    def save_report(self, output_path: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç –≤ JSON"""
        with open(output_path, "w") as f:
            json.dump(self.results, f, indent=2)
        print(f"\nüíæ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")


# ========================= Main =========================

def main():
    ap = argparse.ArgumentParser(
        description="–í–∞–ª–∏–¥–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞ —Å —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–æ–π",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    ap.add_argument("--index-dir", default="index")
    ap.add_argument("--test-size", type=int, default=DEFAULT_TEST_SIZE,
                    help="–ö–æ–ª-–≤–æ –∫—Ä–æ–ø–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    ap.add_argument("--quick", action="store_true",
                    help="–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç (–º–µ–Ω—å—à–µ –∫—Ä–æ–ø–æ–≤)")
    ap.add_argument("--report", default="validation_report.json",
                    help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç")
    ap.add_argument("--auto-tune", action="store_true",
                    help="Auto-tuning –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ)")
    
    args = ap.parse_args()
    
    print("=" * 80)
    print("üî¨ –í–ê–õ–ò–î–ê–¶–ò–Ø –ò–ù–î–ï–ö–°–ê")
    print("=" * 80)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞
    meta, embs, index, model_config = load_index(Path(args.index_dir))
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä
    test_size = DEFAULT_QUICK_SIZE if args.quick else args.test_size
    test_set = sample_test_set(meta, test_size)
    
    print(f"\nüìã –¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {len(test_set)} –∫—Ä–æ–ø–æ–≤")
    
    # –í–∞–ª–∏–¥–∞—Ç–æ—Ä
    validator = IndexValidator(meta, embs, index, model_config)
    
    # –¢–µ—Å—Ç—ã
    validator.test_self_retrieval(test_set)
    validator.test_same_pano_retrieval(test_set)
    validator.test_similarity_distribution()
    validator.benchmark_speed(test_set)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
    validator.save_report(args.report)
    
    print("\n" + "=" * 80)
    print("‚úÖ –í–ê–õ–ò–î–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
    print("=" * 80)
    
    # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    self_ret = validator.results.get("test_self_retrieval", {})
    top1_acc = self_ret.get("top1_acc", 0)
    
    print(f"\nüéØ –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞:")
    if top1_acc >= 95:
        print("   ‚úÖ ‚úÖ ‚úÖ –û–¢–õ–ò–ß–ù–û! –ò–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤ –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É")
    elif top1_acc >= 85:
        print("   ‚úÖ ‚úÖ –•–û–†–û–®–û! –ò–Ω–¥–µ–∫—Å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    elif top1_acc >= 70:
        print("   ‚ö†Ô∏è  –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å –∏–Ω–¥–µ–∫—Å")
    else:
        print("   ‚ùå –ü–õ–û–•–û! –ò–Ω–¥–µ–∫—Å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω—É–∂–Ω–∞ –æ—Ç–ª–∞–¥–∫–∞!")


if __name__ == "__main__":
    main()