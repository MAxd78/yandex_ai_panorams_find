#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
04_build_index_production.py ‚Äî Production-ready –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–ª—è vast.ai

–§–∏—á–∏:
  ‚úÖ Checkpointing –∫–∞–∂–¥—ã–µ N —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (resume –ø–æ—Å–ª–µ —Å–±–æ—è)
  ‚úÖ FP16 Mixed Precision (2x —É—Å–∫–æ—Ä–µ–Ω–∏–µ + —ç–∫–æ–Ω–æ–º–∏—è VRAM)
  ‚úÖ Dynamic batch sizing –ø–æ –¥–æ—Å—Ç—É–ø–Ω–æ–π VRAM
  ‚úÖ OOM handling —Å –∞–≤—Ç–æ—É–º–µ–Ω—å—à–µ–Ω–∏–µ–º batch
  ‚úÖ GPU monitoring (—É—Ç–∏–ª–∏–∑–∞—Ü–∏—è, VRAM, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞)
  ‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª —Å —Ä–æ—Ç–∞—Ü–∏–µ–π
  ‚úÖ Graceful shutdown (SIGTERM/SIGINT)
  ‚úÖ ETA —Å —É—á—ë—Ç–æ–º checkpoints
  ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞ –ø–æ—Å–ª–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫
  python scripts/04_build_index_production.py --clip-model "ViT-L-14" --ocr

  # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –ø–æ—Å–ª–µ —Å–±–æ—è
  python scripts/04_build_index_production.py --resume
"""

from __future__ import annotations
import os
import sys
import json
import argparse
import warnings
import time
import signal
import logging
from datetime import datetime
from typing import List, Optional, Tuple
from pathlib import Path

import numpy as np
import pandas as pd
from tqdm import tqdm
from PIL import Image

import torch
import torch.cuda.amp as amp
import open_clip
import hnswlib

# OCR + —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
HAS_OCR = False
try:
    import easyocr
    from sklearn.feature_extraction.text import TfidfVectorizer
    from scipy import sparse
    import joblib
    HAS_OCR = True
except ImportError:
    pass

from multiprocessing import Pool, cpu_count

# –ü–æ–¥–∞–≤–ª—è–µ–º –≤–∞—Ä–Ω–∏–Ω–≥–∏
warnings.filterwarnings("ignore", message=".*pin_memory.*MPS.*")
warnings.filterwarnings("ignore", message=".*QuickGELU.*")

# ========================= –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã =========================
DEFAULT_MODEL = "ViT-L-14"
DEFAULT_PRETRAINED = "openai"
SEED = 42
CHECKPOINT_INTERVAL = 5000  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ 5000 —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
GPU_MONITOR_INTERVAL = 30  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫

# ========================= –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ =========================
GRACEFUL_SHUTDOWN = False
LAST_CHECKPOINT_TIME = time.time()

# ========================= –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ =========================

def setup_logging(log_dir: str = "logs") -> logging.Logger:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ñ–∞–π–ª –∏ stdout"""
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"build_index_{timestamp}.log")
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # File handler
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.DEBUG)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)
    
    # Logger
    logger = logging.getLogger("build_index")
    logger.setLevel(logging.DEBUG)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    logger.info(f"–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª: {log_file}")
    return logger

# ========================= GPU Utilities =========================

def get_gpu_info() -> dict:
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU"""
    if not torch.cuda.is_available():
        return {"available": False}
    
    info = {
        "available": True,
        "device_name": torch.cuda.get_device_name(0),
        "device_count": torch.cuda.device_count(),
        "cuda_version": torch.version.cuda,
    }
    
    try:
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        
        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        info["vram_total"] = mem_info.total / 1024**3  # GB
        info["vram_used"] = mem_info.used / 1024**3
        info["vram_free"] = mem_info.free / 1024**3
        
        try:
            util = pynvml.nvmlDeviceGetUtilizationRates(handle)
            info["gpu_util"] = util.gpu
        except:
            pass
        
        try:
            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            info["temperature"] = temp
        except:
            pass
        
        pynvml.nvmlShutdown()
    except:
        pass
    
    return info


def log_gpu_info(logger: logging.Logger):
    """–õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU"""
    info = get_gpu_info()
    
    if not info["available"]:
        logger.warning("‚ö†Ô∏è  GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞ CPU!")
        return
    
    logger.info(f"üéÆ GPU: {info['device_name']}")
    
    if "vram_total" in info:
        vram_used = info.get("vram_used", 0)
        vram_total = info.get("vram_total", 0)
        vram_pct = (vram_used / vram_total * 100) if vram_total > 0 else 0
        
        gpu_util = info.get("gpu_util", "N/A")
        temp = info.get("temperature", "N/A")
        
        logger.info(
            f"   VRAM: {vram_used:.1f}/{vram_total:.1f} GB ({vram_pct:.1f}%) | "
            f"GPU Util: {gpu_util}% | Temp: {temp}¬∞C"
        )


def get_optimal_batch_size(vram_gb: float, model_name: str, use_fp16: bool = True) -> int:
    """–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ batch size –ø–æ VRAM"""
    
    # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è ViT-L-14
    if "ViT-L" in model_name:
        if use_fp16:
            # FP16: ~80MB –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if vram_gb >= 30:  # RTX 5090
                return 512
            elif vram_gb >= 20:  # A100 40GB, RTX 5080
                return 256
            elif vram_gb >= 14:  # RTX 5080 16GB
                return 192
            elif vram_gb >= 10:  # RTX 5070
                return 128
            else:
                return 64
        else:
            # FP32: ~160MB –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if vram_gb >= 30:
                return 256
            elif vram_gb >= 20:
                return 128
            elif vram_gb >= 14:
                return 96
            else:
                return 48
    
    # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    return 64 if use_fp16 else 32


# ========================= Signal Handlers =========================

def signal_handler(signum, frame):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown"""
    global GRACEFUL_SHUTDOWN
    logger = logging.getLogger("build_index")
    logger.warning(f"\n‚ö†Ô∏è  –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}, —Å–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint –∏ –≤—ã—Ö–æ–¥–∏–º...")
    GRACEFUL_SHUTDOWN = True


# ========================= Checkpoint Management =========================

def save_checkpoint(
    index_dir: Path,
    embeddings: np.ndarray,
    processed_indices: list,
    checkpoint_id: int,
    logger: logging.Logger
):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å checkpoint"""
    checkpoint_dir = index_dir / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)
    
    checkpoint_file = checkpoint_dir / f"checkpoint_{checkpoint_id:08d}.npz"
    
    np.savez_compressed(
        checkpoint_file,
        embeddings=embeddings,
        processed_indices=np.array(processed_indices, dtype=np.int64),
        checkpoint_id=checkpoint_id,
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    meta_file = checkpoint_dir / "checkpoint_latest.json"
    with open(meta_file, "w") as f:
        json.dump({
            "checkpoint_id": checkpoint_id,
            "checkpoint_file": str(checkpoint_file.name),
            "timestamp": datetime.now().isoformat(),
            "num_processed": len(processed_indices),
        }, f, indent=2)
    
    logger.info(f"üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {checkpoint_file.name} ({len(processed_indices)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)")
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ checkpoints (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3)
    checkpoints = sorted(checkpoint_dir.glob("checkpoint_*.npz"))
    if len(checkpoints) > 3:
        for old_cp in checkpoints[:-3]:
            old_cp.unlink()
            logger.debug(f"üóëÔ∏è  –£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π checkpoint: {old_cp.name}")


def load_checkpoint(index_dir: Path, logger: logging.Logger) -> Optional[Tuple[np.ndarray, list, int]]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π checkpoint"""
    checkpoint_dir = index_dir / "checkpoints"
    meta_file = checkpoint_dir / "checkpoint_latest.json"
    
    if not meta_file.exists():
        logger.info("‚ÑπÔ∏è  Checkpoints –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è")
        return None
    
    try:
        with open(meta_file, "r") as f:
            meta = json.load(f)
        
        checkpoint_file = checkpoint_dir / meta["checkpoint_file"]
        
        if not checkpoint_file.exists():
            logger.warning(f"‚ö†Ô∏è  Checkpoint —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {checkpoint_file}")
            return None
        
        data = np.load(checkpoint_file)
        embeddings = data["embeddings"]
        processed_indices = data["processed_indices"].tolist()
        checkpoint_id = int(data["checkpoint_id"])
        
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω checkpoint: {checkpoint_file.name}")
        logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_indices)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        
        return embeddings, processed_indices, checkpoint_id
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ checkpoint: {e}")
        return None


# ========================= Device & Model =========================

def device_auto() -> torch.device:
    """–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º"""
    if torch.cuda.is_available():
        return torch.device("cuda")
    if torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def load_clip(
    model_name: str,
    pretrained: str,
    device: torch.device,
    logger: logging.Logger
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ CLIP –º–æ–¥–µ–ª–∏"""
    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name} ({pretrained})")
    
    try:
        model, _, preprocess = open_clip.create_model_and_transforms(
            model_name, pretrained=pretrained, device=device
        )
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–∫–∞–∑–∞–Ω–∏–∏ device: {e}, –ø—Ä–æ–±—É–µ–º –±–µ–∑ –Ω–µ–≥–æ")
        model, _, preprocess = open_clip.create_model_and_transforms(
            model_name, pretrained=pretrained
        )
        model = model.to(device)
    
    model.eval()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    try:
        embed_dim = model.visual.output_dim
    except AttributeError:
        embed_dim = model.text_projection.shape[-1]
    
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embed_dim}")
    
    return model, preprocess, embed_dim


def normalize_embeddings(embs: np.ndarray) -> np.ndarray:
    """L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    norms = np.linalg.norm(embs, axis=1, keepdims=True) + 1e-9
    return (embs / norms).astype(np.float32)


# ========================= OCR Functions =========================

def _init_reader(langs):
    global _READER
    _READER = easyocr.Reader(list(langs), gpu=False, verbose=False)


def _ocr_one(path: str) -> str:
    try:
        res = _READER.readtext(path, detail=0, paragraph=True, batch_size=16)
        return " ".join([t for t in res if isinstance(t, str)]).strip()
    except Exception:
        return ""


def run_ocr(
    paths: List[str],
    lang=("ru", "en"),
    workers: int = 0,
    chunk: int = 32,
    out_txt_path: str | None = None,
    append: bool = False,
    logger: logging.Logger = None,
) -> List[str]:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π OCR —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
    workers = workers or max(1, cpu_count() // 2)
    texts: List[str] = []

    if not append and out_txt_path and os.path.exists(out_txt_path):
        os.remove(out_txt_path)

    if workers <= 1:
        reader = easyocr.Reader(list(lang), gpu=False, verbose=False)
        for p in tqdm(paths, desc="OCR", unit="img"):
            try:
                res = reader.readtext(p, detail=0, paragraph=True, batch_size=16)
                txt = " ".join([t for t in res if isinstance(t, str)]).strip()
            except Exception:
                txt = ""
            texts.append(txt)
            if out_txt_path:
                with open(out_txt_path, "a", encoding="utf-8") as f:
                    f.write((txt or "") + "\n")
        return texts

    with Pool(processes=workers, initializer=_init_reader, initargs=(list(lang),)) as pool:
        for txt in tqdm(
            pool.imap(_ocr_one, paths, chunksize=chunk),
            total=len(paths),
            desc="OCR",
            unit="img",
        ):
            texts.append(txt)
            if out_txt_path:
                with open(out_txt_path, "a", encoding="utf-8") as f:
                    f.write((txt or "") + "\n")
    
    return texts


# ========================= Main =========================

def main():
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    ap = argparse.ArgumentParser(
        description="Production-ready –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–ª—è vast.ai",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    ap.add_argument("--crops-meta", default="meta/crops.csv")
    ap.add_argument("--outdir", default="index")
    
    # CLIP –º–æ–¥–µ–ª—å
    ap.add_argument("--clip-model", default=DEFAULT_MODEL)
    ap.add_argument("--clip-ckpt", default=DEFAULT_PRETRAINED)
    ap.add_argument("--batch", type=int, default=0,
                    help="Batch size (0=auto –ø–æ VRAM)")
    
    # FP16
    ap.add_argument("--fp16", action="store_true", default=True,
                    help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mixed precision FP16 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)")
    ap.add_argument("--no-fp16", action="store_false", dest="fp16",
                    help="–û—Ç–∫–ª—é—á–∏—Ç—å FP16 (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –±–æ–ª—å—à–µ VRAM)")
    
    # HNSW
    ap.add_argument("--hnsw-M", dest="hnsw_M", type=int, default=32)
    ap.add_argument("--hnsw-efC", dest="hnsw_efC", type=int, default=200)
    ap.add_argument("--no-hnsw", action="store_true")
    
    # OCR
    ap.add_argument("--ocr", action="store_true")
    ap.add_argument("--ocr-workers", type=int, default=0)
    ap.add_argument("--ocr-chunk", type=int, default=32)
    
    # Resume
    ap.add_argument("--resume", action="store_true",
                    help="–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ checkpoint")
    ap.add_argument("--checkpoint-interval", type=int, default=CHECKPOINT_INTERVAL,
                    help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å checkpoint –∫–∞–∂–¥—ã–µ N —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    
    # –õ–æ–≥–∏
    ap.add_argument("--log-dir", default="logs")
    
    args = ap.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger = setup_logging(args.log_dir)
    
    logger.info("=" * 80)
    logger.info("üöÄ PRODUCTION BUILD INDEX - –ó–ê–ü–£–°–ö")
    logger.info("=" * 80)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ OCR –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    if args.ocr and not HAS_OCR:
        logger.error("‚ùå OCR –∑–∞–ø—Ä–æ—à–µ–Ω, –Ω–æ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã:")
        logger.error("   pip install easyocr scikit-learn joblib scipy")
        sys.exit(1)
    
    os.makedirs(args.outdir, exist_ok=True)
    
    # ============= GPU Info =============
    logger.info("\n" + "=" * 80)
    logger.info("üíª –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –°–ò–°–¢–ï–ú–ï")
    logger.info("=" * 80)
    
    gpu_info = get_gpu_info()
    log_gpu_info(logger)
    
    device = device_auto()
    logger.info(f"üéØ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    use_cuda = device.type == "cuda"
    use_fp16 = args.fp16 and use_cuda
    
    if use_fp16:
        logger.info("‚ö° Mixed Precision: FP16 ENABLED (2x —É—Å–∫–æ—Ä–µ–Ω–∏–µ)")
    else:
        logger.info("‚ÑπÔ∏è  Mixed Precision: FP32 (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ)")
    
    # ============= –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö =============
    logger.info("\n" + "=" * 80)
    logger.info("üìÇ –ó–ê–ì–†–£–ó–ö–ê –ú–ï–¢–ê–î–ê–ù–ù–´–•")
    logger.info("=" * 80)
    
    if not os.path.exists(args.crops_meta):
        logger.error(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {args.crops_meta}")
        logger.error("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/03_prepare_dataset.py")
        sys.exit(1)
    
    df = pd.read_csv(args.crops_meta)
    needed = {"path", "crop_id", "pano_id", "lat", "lon"}
    missing = needed - set(df.columns)
    if missing:
        logger.error(f"‚ùå –í {args.crops_meta} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing}")
        sys.exit(1)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤...")
    valid_mask = df["path"].apply(os.path.exists)
    n_missing = (~valid_mask).sum()
    
    if n_missing > 0:
        logger.warning(f"‚ö†Ô∏è  {n_missing} —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –æ–Ω–∏ –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã")
        df = df[valid_mask].reset_index(drop=True)
    
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∫—Ä–æ–ø–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ø–∏—é –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    df.to_parquet(os.path.join(args.outdir, "crops.parquet"), index=False)
    
    paths = df["path"].tolist()
    
    # ============= –ü—Ä–æ–≤–µ—Ä–∫–∞ resume =============
    index_dir = Path(args.outdir)
    checkpoint_data = None
    start_idx = 0
    
    if args.resume:
        logger.info("\nüîÑ –ü–æ–∏—Å–∫ checkpoints –¥–ª—è resume...")
        checkpoint_data = load_checkpoint(index_dir, logger)
        
        if checkpoint_data is not None:
            existing_embs, processed_indices, checkpoint_id = checkpoint_data
            start_idx = len(processed_indices)
            logger.info(f"‚úÖ Resume —Å –ø–æ–∑–∏—Ü–∏–∏: {start_idx}/{len(paths)}")
    
    # ============= –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ =============
    logger.info("\n" + "=" * 80)
    logger.info("ü§ñ –ó–ê–ì–†–£–ó–ö–ê CLIP –ú–û–î–ï–õ–ò")
    logger.info("=" * 80)
    
    torch.manual_seed(SEED)
    np.random.seed(SEED)
    
    model, preprocess, embed_dim = load_clip(args.clip_model, args.clip_ckpt, device, logger)
    
    # ============= –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ batch size =============
    if args.batch == 0:
        vram_gb = gpu_info.get("vram_total", 12)
        optimal_batch = get_optimal_batch_size(vram_gb, args.clip_model, use_fp16)
        logger.info(f"üéØ Auto batch size: {optimal_batch} (VRAM: {vram_gb:.1f} GB)")
        batch_size = optimal_batch
    else:
        batch_size = args.batch
        logger.info(f"üì¶ Manual batch size: {batch_size}")
    
    # ============= –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ =============
    logger.info("\n" + "=" * 80)
    logger.info("üßÆ –í–´–ß–ò–°–õ–ï–ù–ò–ï CLIP –≠–ú–ë–ï–î–î–ò–ù–ì–û–í")
    logger.info("=" * 80)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Å–∏–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    if checkpoint_data is not None:
        embs = checkpoint_data[0]
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(embs)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ checkpoint")
    else:
        embs = np.zeros((len(paths), embed_dim), dtype=np.float32)
    
    # Scaler –¥–ª—è FP16
    scaler = amp.GradScaler() if use_fp16 else None
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    last_gpu_log = time.time()
    processed = start_idx
    errors = 0
    last_checkpoint_idx = (start_idx // args.checkpoint_interval) * args.checkpoint_interval
    
    # Progress bar
    pbar = tqdm(
        total=len(paths),
        initial=start_idx,
        desc="CLIP",
        unit="img",
        ncols=100,
    )
    
    try:
        with torch.no_grad():
            idx = start_idx
            
            while idx < len(paths) and not GRACEFUL_SHUTDOWN:
                # –ë–∞—Ç—á –ø—É—Ç–µ–π
                batch_end = min(idx + batch_size, len(paths))
                batch_paths = paths[idx:batch_end]
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                ims = []
                for p in batch_paths:
                    try:
                        img = Image.open(p).convert("RGB")
                        ims.append(preprocess(img))
                    except Exception as e:
                        logger.debug(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {p}: {e}")
                        ims.append(preprocess(Image.new("RGB", (224, 224), (0, 0, 0))))
                        errors += 1
                
                ims_tensor = torch.stack(ims, dim=0).to(device)
                
                # Forward pass —Å FP16 (–µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω)
                try:
                    if use_fp16:
                        with amp.autocast():
                            feats = model.encode_image(ims_tensor)
                    else:
                        feats = model.encode_image(ims_tensor)
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                    feats = torch.nn.functional.normalize(feats, dim=-1)
                    feats_np = feats.detach().cpu().numpy().astype(np.float32)
                    
                    embs[idx:batch_end] = feats_np
                    processed = batch_end
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        logger.warning(f"‚ö†Ô∏è  OOM! –£–º–µ–Ω—å—à–∞–µ–º batch: {batch_size} -> {batch_size // 2}")
                        torch.cuda.empty_cache()
                        batch_size = max(batch_size // 2, 4)
                        continue
                    else:
                        raise
                
                # Checkpoint
                if processed - last_checkpoint_idx >= args.checkpoint_interval:
                    checkpoint_id = processed // args.checkpoint_interval
                    save_checkpoint(
                        index_dir,
                        embs[:processed],
                        list(range(processed)),
                        checkpoint_id,
                        logger
                    )
                    last_checkpoint_idx = processed
                
                # GPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
                if time.time() - last_gpu_log > GPU_MONITOR_INTERVAL:
                    log_gpu_info(logger)
                    last_gpu_log = time.time()
                
                # Update progress
                pbar.update(len(batch_paths))
                idx = batch_end
    
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º, —Å–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint...")
        GRACEFUL_SHUTDOWN = True
    
    finally:
        pbar.close()
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π checkpoint –µ—Å–ª–∏ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ
        if processed < len(paths):
            logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ checkpoint...")
            save_checkpoint(
                index_dir,
                embs[:processed],
                list(range(processed)),
                processed // args.checkpoint_interval + 1,
                logger
            )
    
    if GRACEFUL_SHUTDOWN:
        logger.info("üõë Graceful shutdown –∑–∞–≤–µ—Ä—à—ë–Ω")
        sys.exit(0)
    
    logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}/{len(paths)} ({errors} –æ—à–∏–±–æ–∫)")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    embs = normalize_embeddings(embs)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    embs_path = os.path.join(args.outdir, "embs.npy")
    np.save(embs_path, embs)
    logger.info(f"üíæ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {embs_path} (shape: {embs.shape})")
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
    model_meta = {
        "model": args.clip_model,
        "pretrained": args.clip_ckpt,
        "embed_dim": int(embed_dim),
        "tile_size": 336,
        "tile_stride": 224,
        "seed": SEED,
        "fp16": use_fp16,
        "version": "3.0-production",
        "created_at": datetime.now().isoformat(),
    }
    
    model_json_path = os.path.join(args.outdir, "model.json")
    with open(model_json_path, "w") as f:
        json.dump(model_meta, f, indent=2)
    logger.info(f"üìù –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {model_json_path}")
    
    # ============= HNSW –∏–Ω–¥–µ–∫—Å =============
    if not args.no_hnsw:
        logger.info("\n" + "=" * 80)
        logger.info("üîó –ü–û–°–¢–†–û–ï–ù–ò–ï HNSW –ò–ù–î–ï–ö–°–ê")
        logger.info("=" * 80)
        
        index = hnswlib.Index(space="cosine", dim=embed_dim)
        index.init_index(
            max_elements=embs.shape[0],
            M=args.hnsw_M,
            ef_construction=args.hnsw_efC,
            random_seed=SEED,
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Ä—Ü–∏—è–º–∏
        batch_size = 10000
        for i in tqdm(range(0, len(embs), batch_size), desc="HNSW", unit="batch"):
            end = min(i + batch_size, len(embs))
            index.add_items(embs[i:end], np.arange(i, end, dtype=np.int64))
        
        hnsw_path = os.path.join(args.outdir, "hnsw.bin")
        index.save_index(hnsw_path)
        logger.info(f"‚úÖ HNSW —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {hnsw_path}")
    
    # ============= OCR =============
    if args.ocr:
        logger.info("\n" + "=" * 80)
        logger.info("üìù OCR –ò TF-IDF")
        logger.info("=" * 80)
        
        ocr_txt_path = os.path.join(args.outdir, "ocr_texts.txt")
        
        logger.info("üîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ OCR...")
        texts = run_ocr(
            paths,
            lang=("ru", "en"),
            workers=args.ocr_workers,
            chunk=args.ocr_chunk,
            out_txt_path=ocr_txt_path,
            logger=logger,
        )
        
        logger.info(f"‚úÖ OCR –∑–∞–≤–µ—Ä—à—ë–Ω: {ocr_txt_path}")
        
        # TF-IDF
        logger.info("üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ TF-IDF –∏–Ω–¥–µ–∫—Å–∞...")
        vect = TfidfVectorizer(
            lowercase=True,
            analyzer="word",
            token_pattern=r"(?u)\b[\w\-]{2,}\b",
            ngram_range=(1, 2),
            max_features=200_000,
            min_df=1,
            max_df=0.95,
        )
        X = vect.fit_transform(texts)
        
        joblib.dump(vect, os.path.join(args.outdir, "tfidf_vectorizer.joblib"))
        sparse.save_npz(os.path.join(args.outdir, "tfidf_matrix.npz"), X)
        
        logger.info(f"‚úÖ TF-IDF –∏–Ω–¥–µ–∫—Å (vocabulary: {len(vect.vocabulary_)})")
    
    # ============= –§–∏–Ω–∞–ª =============
    logger.info("\n" + "=" * 80)
    logger.info("‚úÖ –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
    logger.info("=" * 80)
    logger.info(f"üìÅ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {args.outdir}/")
    logger.info(f"   - embs.npy ({embs.shape[0]} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, dim={embs.shape[1]})")
    
    if not args.no_hnsw:
        logger.info(f"   - hnsw.bin (M={args.hnsw_M}, efC={args.hnsw_efC})")
    
    logger.info(f"   - model.json (–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)")
    
    if args.ocr:
        logger.info(f"   - ocr_texts.txt + TF-IDF –∏–Ω–¥–µ–∫—Å")
    
    logger.info("\nüéØ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–∏—Å–∫:")
    logger.info("   python scripts/05_query.py --image samples/query.jpg")
    
    # –£–¥–∞–ª—è–µ–º checkpoints –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    checkpoint_dir = index_dir / "checkpoints"
    if checkpoint_dir.exists():
        import shutil
        shutil.rmtree(checkpoint_dir)
        logger.info("üóëÔ∏è  Checkpoints —É–¥–∞–ª–µ–Ω—ã (–∏–Ω–¥–µ–∫—Å –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤)")


if __name__ == "__main__":
    main()